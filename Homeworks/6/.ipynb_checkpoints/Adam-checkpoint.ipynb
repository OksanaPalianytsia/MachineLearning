{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    NN for binary classification\n",
    "    Attributes:\n",
    "    ...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layers_d, normalize = True, learning_rate = 0.01, num_iter = 30000, epsilon = (10)^(-8), betha_1 = 0.9, betha_2 = 0.999, k = 500):\n",
    "        self.layers_d = layers_d # тут лише приховані шари а 0-го та останнього (з одним нейроном) немає\n",
    "        self.L = len(self.layers_d) + 1 # кількість шарів нейронів в мережі без урахування вихідного\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.normalize = normalize\n",
    "        self.epsilon = epsilon\n",
    "        self.betha_1 = betha_1\n",
    "        self.betha_2 = betha_2\n",
    "        self.k = k\n",
    "    \n",
    "    def __normalize(self, X, mean = None, std = None):\n",
    "        \"\"\"\n",
    "        Зверніть увагу, що нормалізація вхідних даних є дуже важливою для швидкодії нейронних мереж.\n",
    "        \"\"\"\n",
    "        '''\n",
    "        X.shape =  (n, m)\n",
    "        '''\n",
    "        n = X.shape[0]\n",
    "        m = mean\n",
    "        if m is None:\n",
    "            m = np.mean(X, axis=1).reshape((n, 1))\n",
    "            '''\n",
    "            m.shape =  (n, 1)\n",
    "            '''\n",
    "        s = std\n",
    "        if s is None:\n",
    "            s = np.std(X, axis=1).reshape((n, 1))\n",
    "            '''\n",
    "            s.shape =  (n, 1)\n",
    "            '''\n",
    "        X_new = (X - m) / s\n",
    "        return X_new, m, s\n",
    "\n",
    "    def __sigmoid(self, Z):\n",
    "        \"\"\"\n",
    "        В наступних практичних потрібно буде додати підтримку й інших активаційних функцій - це один з гіперпараметрів. \n",
    "        Їх можна вибирати для всіх шарів одночасно або мати різні активаційні функції на кожному з них.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-Z))\n",
    "    \n",
    "    def __softmax(self, Z):\n",
    "        \n",
    "        x = np.exp(Z)        \n",
    "        '''\n",
    "        Z_i.shape  = (n_l, 1)\n",
    "        x.shape = (n_l, 1)\n",
    "        '''\n",
    "        return x / np.sum(x, axis=0, keepdims = True)\n",
    "    \n",
    "    def __initialize_parameters(self):\n",
    "        \n",
    "        self.parameters = {} \n",
    "        # стоврюємо словник зі значеннями W_i та b_i,ключами в якому будуть назви W_1, w_2, ... та b_1, b_2 і т.д\n",
    "        self.adam = {}\n",
    "        \n",
    "            \n",
    "        for i in range(1, self.L + 1):\n",
    "            \n",
    "            self.adam['VdW_' + str(i)] = 0\n",
    "            self.adam['SdW_' + str(i)] = 0\n",
    "            \n",
    "            self.adam['Vdb_' + str(i)] = 0            \n",
    "            self.adam['Sdb_' + str(i)] = 0\n",
    "\n",
    "            self.parameters['W_' + str(i)] = np.random.randn(self.layers_d[i], self.layers_d[i - 1])* np.sqrt(2/self.layers_d[i - 1])\n",
    "            '''\n",
    "            W_i.shape  = (n_l, n_l-1) # (кількість нейронів на поточному шарі, кількість на попередньому)\n",
    "            '''\n",
    "            self.parameters['b_' + str(i)] = np.zeros((self.layers_d[i],1))\n",
    "            '''\n",
    "            b_i.shape  = (n_l,1) # (кількість нейронів на поточному шарі, 1)\n",
    "            '''\n",
    "       \n",
    "    def __forward_propagation(self, X):\n",
    "        \n",
    "        cache = {} # стоврюємо словник зі значеннями Z_i та A_i,ключами в якому будуть назви A_0, A_1, A_2, ... та Z_1, Z_2 і т.д\n",
    "        cache['A_0'] = X\n",
    "        \n",
    "        for i in range(1, self.L):\n",
    "            cache['Z_' + str(i)] = np.dot(self.parameters['W_' + str(i)], cache['A_' + str(i - 1)]) + self.parameters['b_' + str(i)]\n",
    "            '''\n",
    "            Z_i.shape  = (n_l, 1) = (n_l, n_l-1) * (n_l-1, 1) + (n_l,1)\n",
    "            '''\n",
    "            cache['A_' + str(i)] = self.__sigmoid(cache['Z_' + str(i)])\n",
    "            '''\n",
    "            A_i.shape  = (n_l, 1) = (n_l, 1)\n",
    "            '''       \n",
    "        cache['Z_' + str(self.L)] = np.dot(self.parameters['W_' + str(self.L)], cache['A_' + str(self.L - 1)]) + self.parameters['b_' + str(self.L)]\n",
    "        cache['A_' + str(self.L)] = self.__softmax(cache['Z_' + str(self.L)])\n",
    "        '''\n",
    "        функція softmax на останньому кроці \n",
    "        ''' \n",
    "        return cache['A_' + str(self.L)], cache\n",
    "    \n",
    "    def compute_cost(self, A, Y):\n",
    "        m = Y.shape[1]\n",
    "        res = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "        J = -(1 / m) * np.sum(res)\n",
    "        '''\n",
    "        J.shape  = sum((1, m) x (1, m) - (1, m) x (1, m)) = sum((1, m)) = (1, 1)\n",
    "        '''\n",
    "        return J\n",
    "        \n",
    "    def __backward_propagation(self, X, Y, cache):\n",
    "        \n",
    "        m = X.shape[1]\n",
    "        gradients = {}\n",
    "        \n",
    "        gradients['dZ_' + str(self.L)] = cache['A_' + str(self.L)] - Y\n",
    "        '''\n",
    "        dZ_L.shape  = (1, m) - (1, m) = (1, m)\n",
    "        '''\n",
    "        gradients['dW_' + str(self.L)] = (1/m) * np.dot (gradients['dZ_' + str(self.L)], cache['A_' + str(self.L - 1)].T)\n",
    "        '''\n",
    "        dW_L.shape  = (1, m) * ((n_l-1, m).T) = (1, m) * (m, n_l-1) = (1, n_l-1)\n",
    "        '''\n",
    "        gradients['db_' + str(self.L)] = (1/m) * np.sum(gradients['dZ_' + str(self.L)], axis = 1, keepdims = True)\n",
    "        '''\n",
    "        db_L.shape  = sum((1, m)) = (1, 1)\n",
    "        '''\n",
    "        \n",
    "        for i in range(self.L - 1, 0, -1):\n",
    "            dA_i = np.dot (self.parameters['W_' + str(i + 1)].T, gradients['dZ_' + str(i + 1)])\n",
    "            '''\n",
    "            dA_i.shape  = (n_l-1, n_l)*(n_l, m) = (n_l-1, m)\n",
    "            '''\n",
    "            gradients['dZ_' + str(i)] = np.multiply(dA_i, cache['A_' + str(i)] * (1 - cache['A_' + str(i)]))\n",
    "            '''\n",
    "            dZ_i.shape  = (n_l, m)x(n_l, m) = (n_l, m)\n",
    "            '''\n",
    "            gradients['dW_' + str(i)] = (1/m) * np.dot (gradients['dZ_' + str(i)], cache['A_' + str(i - 1)].T)\n",
    "            '''\n",
    "            dW_i.shape  = (n_l, m)*((n_l-1, n).T) = (n_l, m)*(m, n_l-1) = (n_l, n_l-1)\n",
    "            '''\n",
    "            gradients['db_' + str(i)] = (1/m) * np.sum(gradients['dZ_' + str(i)], axis = 1, keepdims = True)\n",
    "            '''\n",
    "            db_i.shape = sum((n_l, m)) = (n_l, 1)\n",
    "            '''       \n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def __update_parameters(self, gradients, t):\n",
    "          \n",
    "        for i in range(1, self.L + 1):            \n",
    "            \n",
    "            self.adam['VdW_' + str(i)] = np.multiply(self.betha_1, self.adam['VdW_' + str(i)]) + (1 - self.betha_1) * gradients['dW_' + str(i)]  \n",
    "            self.adam['SdW_' + str(i)] = np.multiply(self.betha_2, self.adam['SdW_' + str(i)]) + (1 - self.betha_2) * np.power(gradients['dW_' + str(i)], 2)\n",
    "            \n",
    "            self.adam['VdW_corr_' + str(i)] = (self.adam['VdW_' + str(i)]) / (1 - np.power(self.betha_1, (t + 1)))\n",
    "            self.adam['SdW_corr_' + str(i)] = (self.adam['SdW_' + str(i)]) / (1 - np.power(self.betha_2, (t + 1)))\n",
    "            \n",
    "            self.parameters['W_' + str(i)] = self.parameters['W_' + str(i)] - (self.learning_rate * self.adam['VdW_corr_' + str(i)] ) / (np.sqrt(self.adam['SdW_corr_' + str(i)]) + self.epsilon)\n",
    "            '''\n",
    "            #W_i.shape  = (n_l, n_l-1) # (кількість нейронів на поточному шарі, кількість на попередньому)\n",
    "            '''\n",
    "\n",
    "            self.adam['Vdb_' + str(i)] =  np.multiply(self.betha_1, self.adam['Vdb_' + str(i)]) + (1 - self.betha_1) * gradients['db_' + str(i)]  \n",
    "            self.adam['Sdb_' + str(i)] =  np.multiply(self.betha_2, self.adam['Sdb_' + str(i)]) + (1 - self.betha_2) * np.power(gradients['db_' + str(i)], 2)          \n",
    "            \n",
    "            self.adam['Vdb_corr_' + str(i)] = (self.adam['Vdb_' + str(i)]) / (1 - np.power(self.betha_1, (t + 1)))          \n",
    "            self.adam['Sdb_corr_' + str(i)] = (self.adam['Sdb_' + str(i)]) / (1 - np.power(self.betha_2, (t + 1)))\n",
    "\n",
    "            self.parameters['b_' + str(i)] = self.parameters['b_' + str(i)] - (self.learning_rate * self.adam['Vdb_corr_' + str(i)]) / (np.sqrt(self.adam['Sdb_corr_' + str(i)]) + self.epsilon)\n",
    "            \n",
    "            '''\n",
    "            #b_i.shape  = (n_l,1) # (кількість нейронів на поточному шарі, 1)\n",
    "            '''\n",
    "            '''\n",
    "            \n",
    "            self.parameters['W_' + str(i)] -= self.learning_rate * gradients['dW_' + str(i)]\n",
    "            self.parameters['b_' + str(i)] -= self.learning_rate * gradients['db_' + str(i)]\n",
    "            '''\n",
    "           \n",
    "        \n",
    "    \n",
    "    def fit(self, X_vert, Y_vert, print_cost = True):\n",
    "        X= X_vert.T\n",
    "        \n",
    "        n_x = X.shape[0] # визначаємо кількість нейронів у вихідному шарі\n",
    "                \n",
    "        lb = LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False) \n",
    "        # задаємо перетворення приналежності до класів як 0 і 1 відвідно для кожного прикладу \n",
    "        #([0, 0, 1] - приклад з класу 2, [1, 0, 0] - приклад з класу 0, [0, 1, 0] - приклад з класу 1)\n",
    "        lb.fit(Y_vert)\n",
    "        \n",
    "        Y = lb.transform(Y_vert).T        \n",
    "        final_classes = Y.shape[0] # визначаємо кількість нейронів у вихідному шарі\n",
    "        \n",
    "        self.layers_d.insert(0, n_x)\n",
    "        self.layers_d.append(final_classes) \n",
    "        '''\n",
    "        додаємо вхідний та вихідний шари до прихованих \n",
    "        і отримуємо клькість всіх шарів нейронної мережі і кількість нейронів на кожному шарі\n",
    "        '''\n",
    "        \n",
    "        if self.normalize:\n",
    "            X, self.__mean, self.__std = self.__normalize(X)\n",
    "        \n",
    "        costs = []\n",
    "        \n",
    "        m = X.shape[1]\n",
    "        n_x = X.shape[0]\n",
    "        \n",
    "        self.__initialize_parameters()\n",
    "        \n",
    "        previous_cost = 0;\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            \n",
    "            A, cache = self.__forward_propagation(X)\n",
    "\n",
    "            cost = self.compute_cost(A, Y)\n",
    "\n",
    "            gradients = self.__backward_propagation(X, Y, cache)\n",
    "\n",
    "            self.__update_parameters(gradients, i)\n",
    "\n",
    "            if print_cost and i % 1 == 0:\n",
    "                print(\"{}-th iteration: {}\".format(i, cost))\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                costs.append(cost)\n",
    "            if (abs(previous_cost - cost) < self.epsilon):\n",
    "                k = k - 1\n",
    "                if (k == 0):\n",
    "                    break;\n",
    "\n",
    "        if print_cost:\n",
    "            plt.plot(costs)\n",
    "            plt.ylabel(\"Cost\")\n",
    "            plt.xlabel(\"Iteration, *1\")\n",
    "            plt.show()  \n",
    "            \n",
    "           \n",
    "    def predict_proba(self, X_vert):\n",
    "        X = X_vert.T\n",
    "        if self.normalize:\n",
    "            X, _, _ = self.__normalize(X, self.__mean, self.__std)    \n",
    "        \n",
    "        probs = self.__forward_propagation(X)[0]\n",
    "        return probs.T\n",
    "    \n",
    "    def predict(self, X_vert):\n",
    "        probs = self.predict_proba(X_vert)\n",
    "        results_bin = (probs == probs.max(axis=1)[:, None]).astype(int)\n",
    "        '''\n",
    "        максимальне значення в кожному рядку перетворюємо на 1 а інші задаємо як 0\n",
    "        наприклад, \n",
    "        a = np.array([[0, 1], [2, 2], [4, 3]])\n",
    "        (a == a.max(axis=1)[:,None]).astype(int)\n",
    "        \n",
    "        Результат : \n",
    "        array([[0, 1], [1, 1], [1, 0]])\n",
    "        '''\n",
    "        return results_bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, Y = load_iris(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer(sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = lb.fit(Y) # перетворюємо значення Y масив значеннь з 0 та 1, де 1 - приналежність до відповідного по порядку класу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = NeuralNet([4, 3, 3],normalize = True, learning_rate = 0.05, betha_1 = 0.9, betha_2 = 0.999, num_iter = 100, epsilon = (10)^(-8), k = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th iteration: 2.137214724506336\n",
      "1-th iteration: 2.1379265503204947\n",
      "2-th iteration: 2.1386398402952946\n",
      "3-th iteration: 2.139354614929965\n",
      "4-th iteration: 2.1400708947614437\n",
      "5-th iteration: 2.140788700324833\n",
      "6-th iteration: 2.1415080521145864\n",
      "7-th iteration: 2.1422289705467463\n",
      "8-th iteration: 2.14295147592248\n",
      "9-th iteration: 2.1436755883931933\n",
      "10-th iteration: 2.144401327927441\n",
      "11-th iteration: 2.145128714279838\n",
      "12-th iteration: 2.1458577669621457\n",
      "13-th iteration: 2.146588505216682\n",
      "14-th iteration: 2.1473209479921587\n",
      "15-th iteration: 2.1480551139220343\n",
      "16-th iteration: 2.1487910213054238\n",
      "17-th iteration: 2.149528688090599\n",
      "18-th iteration: 2.150268131861049\n",
      "19-th iteration: 2.1510093698240826\n",
      "20-th iteration: 2.151752418801915\n",
      "21-th iteration: 2.152497295225133\n",
      "22-th iteration: 2.1532440151284717\n",
      "23-th iteration: 2.1539925941487565\n",
      "24-th iteration: 2.1547430475249008\n",
      "25-th iteration: 2.155495390099808\n",
      "26-th iteration: 2.156249636324035\n",
      "27-th iteration: 2.157005800261058\n",
      "28-th iteration: 2.1577638955939933\n",
      "29-th iteration: 2.158523935633611\n",
      "30-th iteration: 2.1592859333274874\n",
      "31-th iteration: 2.1600499012701464\n",
      "32-th iteration: 2.160815851714044\n",
      "33-th iteration: 2.161583796581254\n",
      "34-th iteration: 2.162353747475722\n",
      "35-th iteration: 2.163125715695966\n",
      "36-th iteration: 2.1638997122481003\n",
      "37-th iteration: 2.1646757478590826\n",
      "38-th iteration: 2.1654538329900843\n",
      "39-th iteration: 2.166233977849886\n",
      "40-th iteration: 2.167016192408236\n",
      "41-th iteration: 2.167800486409078\n",
      "42-th iteration: 2.16858686938361\n",
      "43-th iteration: 2.1693753506630986\n",
      "44-th iteration: 2.1701659393914166\n",
      "45-th iteration: 2.1709586445372566\n",
      "46-th iteration: 2.171753474905994\n",
      "47-th iteration: 2.172550439151171\n",
      "48-th iteration: 2.1733495457855816\n",
      "49-th iteration: 2.1741508031919374\n",
      "50-th iteration: 2.1749542196331193\n",
      "51-th iteration: 2.17575980326199\n",
      "52-th iteration: 2.1765675621307783\n",
      "53-th iteration: 2.177377504200032\n",
      "54-th iteration: 2.17818963734714\n",
      "55-th iteration: 2.179003969374442\n",
      "56-th iteration: 2.1798205080169084\n",
      "57-th iteration: 2.1806392609494374\n",
      "58-th iteration: 2.1814602357937476\n",
      "59-th iteration: 2.1822834401248947\n",
      "60-th iteration: 2.183108881477424\n",
      "61-th iteration: 2.1839365673511715\n",
      "62-th iteration: 2.1847665052167295\n",
      "63-th iteration: 2.1855987025205863\n",
      "64-th iteration: 2.1864331666899663\n",
      "65-th iteration: 2.1872699051373687\n",
      "66-th iteration: 2.1881089252648414\n",
      "67-th iteration: 2.1889502344679794\n",
      "68-th iteration: 2.1897938401396853\n",
      "69-th iteration: 2.190639749673694\n",
      "70-th iteration: 2.1914879704678705\n",
      "71-th iteration: 2.1923385099273105\n",
      "72-th iteration: 2.193191375467239\n",
      "73-th iteration: 2.194046574515729\n",
      "74-th iteration: 2.194904114516249\n",
      "75-th iteration: 2.1957640029300536\n",
      "76-th iteration: 2.1966262472384175\n",
      "77-th iteration: 2.197490854944741\n",
      "78-th iteration: 2.1983578335765155\n",
      "79-th iteration: 2.199227190687176\n",
      "80-th iteration: 2.2000989338578365\n",
      "81-th iteration: 2.2009730706989235\n",
      "82-th iteration: 2.2018496088517137\n",
      "83-th iteration: 2.2027285559897813\n",
      "84-th iteration: 2.203609919820359\n",
      "85-th iteration: 2.204493708085629\n",
      "86-th iteration: 2.2053799285639366\n",
      "87-th iteration: 2.2062685890709437\n",
      "88-th iteration: 2.207159697460717\n",
      "89-th iteration: 2.208053261626764\n",
      "90-th iteration: 2.208949289503017\n",
      "91-th iteration: 2.20984778906477\n",
      "92-th iteration: 2.2107487683295686\n",
      "93-th iteration: 2.211652235358067\n",
      "94-th iteration: 2.212558198254847\n",
      "95-th iteration: 2.21346666516919\n",
      "96-th iteration: 2.214377644295843\n",
      "97-th iteration: 2.2152911438757377\n",
      "98-th iteration: 2.216207172196687\n",
      "99-th iteration: 2.2171257375940643\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_1 \t \n",
      " [[-0.35036609  0.77396083  0.44486411  1.2086005 ]\n",
      " [-0.33370613  0.08092624 -1.60512363 -0.40852225]\n",
      " [ 1.3944958   0.02878048 -0.25624227 -0.56334344]\n",
      " [-0.34444621  0.43752591 -0.04274376 -0.86873156]] \n",
      "b_1 \t \n",
      " [[ 0.00106764]\n",
      " [-0.00097567]\n",
      " [-0.00035735]\n",
      " [ 0.00087175]] \n",
      "W_2 \t \n",
      " [[-0.73278357  0.92844122 -0.22244634 -0.61306172]\n",
      " [-0.77398207 -0.75664857  0.2054195  -0.61477379]\n",
      " [-0.83994818 -0.02545937  0.22415631  0.01887991]] \n",
      "b_2 \t \n",
      " [[ 0.00088006]\n",
      " [-0.00556676]\n",
      " [-0.00201867]] \n",
      "W_3 \t \n",
      " [[ 1.01458453 -1.22811648  0.2557632 ]\n",
      " [-0.41685282  0.15976666 -0.36040366]\n",
      " [-0.99520743 -1.1566736  -1.23195867]] \n",
      "b_3 \t \n",
      " [[0.01455462]\n",
      " [0.01164774]\n",
      " [0.00712998]] \n",
      "W_4 \t \n",
      " [[ 0.28812859  0.99508105  1.37286007]\n",
      " [ 1.31772846  0.34575226  0.15942135]\n",
      " [-0.14317902 -0.09621953  0.13579438]] \n",
      "b_4 \t \n",
      " [[ 0.0307805 ]\n",
      " [ 0.03295732]\n",
      " [-0.06413867]] \n"
     ]
    }
   ],
   "source": [
    "for parameter in cls.parameters:\n",
    "    print(\"{} \\t \\n {} \".format(parameter, cls.parameters[parameter])) #cls.parameters[parameter].shape \\t {} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.40930111, 0.45009128, 0.14060762],\n",
       "       [0.40386108, 0.45468278, 0.14145614],\n",
       "       [0.40620473, 0.45313734, 0.14065793],\n",
       "       [0.40571602, 0.45357688, 0.1407071 ],\n",
       "       [0.4111132 , 0.44878611, 0.14010069],\n",
       "       [0.41862317, 0.4420887 , 0.13928813],\n",
       "       [0.41039432, 0.44988982, 0.13971586],\n",
       "       [0.40844577, 0.45087848, 0.14067575],\n",
       "       [0.40387513, 0.45528706, 0.14083781],\n",
       "       [0.40452942, 0.45410034, 0.14137024],\n",
       "       [0.41170673, 0.4476938 , 0.14059947],\n",
       "       [0.40927672, 0.45045375, 0.14026952],\n",
       "       [0.40361277, 0.45500056, 0.14138666],\n",
       "       [0.40403291, 0.4553308 , 0.14063629],\n",
       "       [0.4144054 , 0.44514744, 0.14044717],\n",
       "       [0.42613855, 0.43572855, 0.1381329 ],\n",
       "       [0.41717822, 0.44343429, 0.13938749],\n",
       "       [0.41031607, 0.44930464, 0.1403793 ],\n",
       "       [0.41426159, 0.44513039, 0.14060802],\n",
       "       [0.41576927, 0.4448909 , 0.13933982],\n",
       "       [0.40795997, 0.45054672, 0.1414933 ],\n",
       "       [0.41527989, 0.44530605, 0.13941406],\n",
       "       [0.4113792 , 0.44917083, 0.13944998],\n",
       "       [0.4101248 , 0.44932973, 0.14054547],\n",
       "       [0.41015201, 0.449611  , 0.14023699],\n",
       "       [0.40407809, 0.45427111, 0.1416508 ],\n",
       "       [0.41071666, 0.44905693, 0.14022642],\n",
       "       [0.40928618, 0.44991466, 0.14079916],\n",
       "       [0.40766366, 0.45124266, 0.14109368],\n",
       "       [0.40688285, 0.45248867, 0.14062848],\n",
       "       [0.40548946, 0.45345784, 0.1410527 ],\n",
       "       [0.40913082, 0.44975944, 0.14110974],\n",
       "       [0.41805801, 0.44285564, 0.13908635],\n",
       "       [0.41991799, 0.4410122 , 0.13906981],\n",
       "       [0.40506132, 0.45369908, 0.1412396 ],\n",
       "       [0.40530888, 0.45347224, 0.14121888],\n",
       "       [0.40794068, 0.45062047, 0.14143885],\n",
       "       [0.41032248, 0.44953226, 0.14014526],\n",
       "       [0.40468984, 0.45467523, 0.14063493],\n",
       "       [0.40817768, 0.45094394, 0.14087837],\n",
       "       [0.41035835, 0.44944891, 0.14019273],\n",
       "       [0.39922439, 0.45883101, 0.14194459],\n",
       "       [0.40697914, 0.45285158, 0.14016929],\n",
       "       [0.41488833, 0.44574026, 0.13937141],\n",
       "       [0.41864547, 0.44239478, 0.13895975],\n",
       "       [0.40454751, 0.45429859, 0.1411539 ],\n",
       "       [0.41474133, 0.44565784, 0.13960083],\n",
       "       [0.40667147, 0.45285401, 0.14047452],\n",
       "       [0.41203177, 0.44758088, 0.14038735],\n",
       "       [0.40690037, 0.45214404, 0.14095559],\n",
       "       [0.43179802, 0.42056938, 0.1476326 ],\n",
       "       [0.43401299, 0.42075282, 0.14523418],\n",
       "       [0.43253735, 0.41963679, 0.14782586],\n",
       "       [0.41385121, 0.43927869, 0.14687009],\n",
       "       [0.42663778, 0.42520052, 0.1481617 ],\n",
       "       [0.42596732, 0.42856062, 0.14547205],\n",
       "       [0.43821593, 0.4174551 , 0.14432896],\n",
       "       [0.40859853, 0.44778793, 0.14361354],\n",
       "       [0.42562336, 0.42599109, 0.14838555],\n",
       "       [0.42232165, 0.43435   , 0.14332834],\n",
       "       [0.40498273, 0.44933612, 0.14568115],\n",
       "       [0.42983876, 0.42590718, 0.14425406],\n",
       "       [0.40986437, 0.44091275, 0.14922287],\n",
       "       [0.42926144, 0.4243831 , 0.14635546],\n",
       "       [0.4214393 , 0.43520236, 0.14335834],\n",
       "       [0.42921307, 0.42374808, 0.14703886],\n",
       "       [0.43284489, 0.42365559, 0.14349952],\n",
       "       [0.41720011, 0.4363719 , 0.14642798],\n",
       "       [0.41670378, 0.43292057, 0.15037564],\n",
       "       [0.41395989, 0.43976686, 0.14627325],\n",
       "       [0.43993162, 0.41674164, 0.14332674],\n",
       "       [0.42116221, 0.43243402, 0.14640377],\n",
       "       [0.42342872, 0.42708649, 0.1494848 ],\n",
       "       [0.42472397, 0.42781494, 0.14746109],\n",
       "       [0.42434441, 0.42832019, 0.1473354 ],\n",
       "       [0.42760828, 0.42506776, 0.14732396],\n",
       "       [0.42545654, 0.42492326, 0.14962021],\n",
       "       [0.4342947 , 0.41839505, 0.14731025],\n",
       "       [0.42962513, 0.42482259, 0.14555228],\n",
       "       [0.4114986 , 0.44291515, 0.14558624],\n",
       "       [0.41191682, 0.44186223, 0.14622095],\n",
       "       [0.41036194, 0.44353845, 0.14609961],\n",
       "       [0.41803997, 0.43611564, 0.14584438],\n",
       "       [0.43056343, 0.42247189, 0.14696469],\n",
       "       [0.43345868, 0.42370778, 0.14283354],\n",
       "       [0.43945987, 0.41787818, 0.14266195],\n",
       "       [0.43224728, 0.42070214, 0.14705058],\n",
       "       [0.4153948 , 0.4341754 , 0.1504298 ],\n",
       "       [0.42737709, 0.42921158, 0.14341133],\n",
       "       [0.41712592, 0.43705146, 0.14582262],\n",
       "       [0.42065253, 0.43335344, 0.14599403],\n",
       "       [0.43060943, 0.42374985, 0.14564071],\n",
       "       [0.41702537, 0.43642503, 0.1465496 ],\n",
       "       [0.40707294, 0.44863861, 0.14428845],\n",
       "       [0.42210181, 0.4325683 , 0.14532989],\n",
       "       [0.42646383, 0.42943573, 0.14410044],\n",
       "       [0.42579406, 0.42970239, 0.14450354],\n",
       "       [0.42496516, 0.42846619, 0.14656866],\n",
       "       [0.40872115, 0.44797565, 0.14330321],\n",
       "       [0.42302904, 0.43199165, 0.14497931],\n",
       "       [0.44596453, 0.40982867, 0.1442068 ],\n",
       "       [0.43486772, 0.41965122, 0.14548106],\n",
       "       [0.43988303, 0.41263081, 0.14748616],\n",
       "       [0.43753977, 0.4160438 , 0.14641644],\n",
       "       [0.44196233, 0.41231363, 0.14572404],\n",
       "       [0.43982365, 0.41178546, 0.14839088],\n",
       "       [0.427997  , 0.428188  , 0.14381501],\n",
       "       [0.43586862, 0.41490817, 0.14922321],\n",
       "       [0.42971259, 0.42028095, 0.15000645],\n",
       "       [0.44645943, 0.4080026 , 0.14553797],\n",
       "       [0.44105297, 0.41383363, 0.1451134 ],\n",
       "       [0.43374391, 0.41873304, 0.14752304],\n",
       "       [0.43971734, 0.4135562 , 0.14672646],\n",
       "       [0.43245155, 0.42160228, 0.14594617],\n",
       "       [0.44052374, 0.41530995, 0.14416632],\n",
       "       [0.4433482 , 0.41204972, 0.14460208],\n",
       "       [0.43808423, 0.4153903 , 0.14652547],\n",
       "       [0.4471707 , 0.40686399, 0.1459653 ],\n",
       "       [0.4366295 , 0.41374282, 0.14962768],\n",
       "       [0.41976127, 0.43009616, 0.15014258],\n",
       "       [0.44330538, 0.41076058, 0.14593404],\n",
       "       [0.43707764, 0.41885087, 0.14407149],\n",
       "       [0.4358811 , 0.41431244, 0.14980646],\n",
       "       [0.43102492, 0.4216037 , 0.14737138],\n",
       "       [0.44385484, 0.41079901, 0.14534615],\n",
       "       [0.44014174, 0.41238165, 0.14747661],\n",
       "       [0.43264892, 0.4209733 , 0.14637777],\n",
       "       [0.43688099, 0.41812121, 0.1449978 ],\n",
       "       [0.43848841, 0.41505506, 0.14645653],\n",
       "       [0.4342971 , 0.41654091, 0.149162  ],\n",
       "       [0.43437581, 0.4158601 , 0.14976408],\n",
       "       [0.44630744, 0.40744962, 0.14624294],\n",
       "       [0.43933959, 0.41444669, 0.14621372],\n",
       "       [0.4301087 , 0.42215493, 0.14773638],\n",
       "       [0.42816416, 0.42311496, 0.14872088],\n",
       "       [0.44015171, 0.41161205, 0.14823624],\n",
       "       [0.44583375, 0.4102441 , 0.14392215],\n",
       "       [0.43995053, 0.4143414 , 0.14570807],\n",
       "       [0.4366755 , 0.4187273 , 0.1445972 ],\n",
       "       [0.4404144 , 0.41301966, 0.14656594],\n",
       "       [0.44305098, 0.41133747, 0.14561155],\n",
       "       [0.44091374, 0.4129264 , 0.14615986],\n",
       "       [0.43486772, 0.41965122, 0.14548106],\n",
       "       [0.4438786 , 0.41041281, 0.14570859],\n",
       "       [0.44498102, 0.40997555, 0.14504343],\n",
       "       [0.44056304, 0.41342023, 0.14601673],\n",
       "       [0.42919489, 0.42253655, 0.14826855],\n",
       "       [0.43878857, 0.4151945 , 0.14601693],\n",
       "       [0.44544653, 0.41093501, 0.14361845],\n",
       "       [0.43841886, 0.41711837, 0.14446277]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_prob = cls.predict_proba(X)\n",
    "Y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 1 1 0 1 0 1 0 0 1 1 1 0 1 1 1\n",
      " 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n"
     ]
    }
   ],
   "source": [
    "Y_hat = cls.predict(X) # отримуємо масив з 0 та 1 \n",
    "Y_hat = np.array(lb.inverse_transform(Y_hat)) # перетворюємо в значення класу, до якого належить приклад\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20666666666666667"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_accuracy = accuracy_score(Y, Y_hat)\n",
    "custom_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes = (20,), max_iter = 10000,  solver = 'adam') #, activation = 'logistic', solver = 'sgd', learning_rate_init = 0.01, learning_rate = 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20,), max_iter=10000)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.93854692e-01, 6.14508225e-03, 2.25512666e-07],\n",
       "       [9.84630633e-01, 1.53680693e-02, 1.29727053e-06],\n",
       "       [9.90968229e-01, 9.03106063e-03, 7.10404384e-07],\n",
       "       [9.83194735e-01, 1.68031147e-02, 2.15044885e-06],\n",
       "       [9.94300661e-01, 5.69911404e-03, 2.25069090e-07],\n",
       "       [9.92304218e-01, 7.69550828e-03, 2.73659899e-07],\n",
       "       [9.90504357e-01, 9.49467488e-03, 9.68515937e-07],\n",
       "       [9.90819342e-01, 9.18017410e-03, 4.83973515e-07],\n",
       "       [9.80794539e-01, 1.92019565e-02, 3.50443474e-06],\n",
       "       [9.85687834e-01, 1.43111690e-02, 9.96911115e-07],\n",
       "       [9.94982448e-01, 5.01744297e-03, 1.09276534e-07],\n",
       "       [9.87521865e-01, 1.24771286e-02, 1.00635797e-06],\n",
       "       [9.86593793e-01, 1.34051726e-02, 1.03448874e-06],\n",
       "       [9.90899556e-01, 9.09936402e-03, 1.08047122e-06],\n",
       "       [9.98395627e-01, 1.60436262e-03, 1.02553826e-08],\n",
       "       [9.98029489e-01, 1.97049036e-03, 2.09285600e-08],\n",
       "       [9.97084183e-01, 2.91575639e-03, 6.02498172e-08],\n",
       "       [9.93354233e-01, 6.64547536e-03, 2.91729890e-07],\n",
       "       [9.92049075e-01, 7.95075125e-03, 1.73999045e-07],\n",
       "       [9.94587325e-01, 5.41247155e-03, 2.03113867e-07],\n",
       "       [9.84769794e-01, 1.52295605e-02, 6.45296960e-07],\n",
       "       [9.93121772e-01, 6.87788392e-03, 3.44190334e-07],\n",
       "       [9.96822658e-01, 3.17718869e-03, 1.53157047e-07],\n",
       "       [9.71266592e-01, 2.87294874e-02, 3.92099954e-06],\n",
       "       [9.73291329e-01, 2.67053457e-02, 3.32485192e-06],\n",
       "       [9.74149833e-01, 2.58475798e-02, 2.58721656e-06],\n",
       "       [9.84070632e-01, 1.59278548e-02, 1.51284569e-06],\n",
       "       [9.92531763e-01, 7.46796867e-03, 2.68689234e-07],\n",
       "       [9.93049885e-01, 6.94987200e-03, 2.43330521e-07],\n",
       "       [9.82063339e-01, 1.79345891e-02, 2.07185198e-06],\n",
       "       [9.78394604e-01, 2.16029172e-02, 2.47857755e-06],\n",
       "       [9.88065998e-01, 1.19334213e-02, 5.80833105e-07],\n",
       "       [9.96848169e-01, 3.15177429e-03, 5.71340348e-08],\n",
       "       [9.97933402e-01, 2.06657552e-03, 2.29254268e-08],\n",
       "       [9.83495277e-01, 1.65032918e-02, 1.43156087e-06],\n",
       "       [9.93572211e-01, 6.42750552e-03, 2.83093515e-07],\n",
       "       [9.95527689e-01, 4.47222935e-03, 8.13664248e-08],\n",
       "       [9.94324337e-01, 5.67544025e-03, 2.22456253e-07],\n",
       "       [9.86483404e-01, 1.35145993e-02, 1.99710274e-06],\n",
       "       [9.90874752e-01, 9.12482523e-03, 4.22566637e-07],\n",
       "       [9.94387929e-01, 5.61181639e-03, 2.54781966e-07],\n",
       "       [9.47210956e-01, 5.27701041e-02, 1.89394030e-05],\n",
       "       [9.89483663e-01, 1.05150186e-02, 1.31860118e-06],\n",
       "       [9.82535193e-01, 1.74625327e-02, 2.27477035e-06],\n",
       "       [9.83830545e-01, 1.61681600e-02, 1.29466993e-06],\n",
       "       [9.82172157e-01, 1.78257094e-02, 2.13312352e-06],\n",
       "       [9.93763383e-01, 6.23639611e-03, 2.20987560e-07],\n",
       "       [9.88442274e-01, 1.15565462e-02, 1.18024706e-06],\n",
       "       [9.94750796e-01, 5.24907119e-03, 1.33129204e-07],\n",
       "       [9.91455247e-01, 8.54431448e-03, 4.38348884e-07],\n",
       "       [3.11416626e-03, 9.86383549e-01, 1.05022852e-02],\n",
       "       [5.78917168e-03, 9.63826818e-01, 3.03840107e-02],\n",
       "       [2.50304187e-03, 9.42105372e-01, 5.53915864e-02],\n",
       "       [6.08690766e-03, 8.71257052e-01, 1.22656040e-01],\n",
       "       [3.08991804e-03, 9.08122835e-01, 8.87872466e-02],\n",
       "       [5.57593430e-03, 8.40512130e-01, 1.53911936e-01],\n",
       "       [5.57288216e-03, 8.98492695e-01, 9.59344230e-02],\n",
       "       [3.59135160e-02, 9.47383146e-01, 1.67033375e-02],\n",
       "       [3.28548864e-03, 9.72467849e-01, 2.42466625e-02],\n",
       "       [1.29636963e-02, 8.86291611e-01, 1.00744693e-01],\n",
       "       [1.06975217e-02, 9.52853750e-01, 3.64487278e-02],\n",
       "       [8.67376120e-03, 9.42249580e-01, 4.90766586e-02],\n",
       "       [4.47443185e-03, 9.78302828e-01, 1.72227404e-02],\n",
       "       [3.90106214e-03, 8.37963378e-01, 1.58135560e-01],\n",
       "       [2.92815365e-02, 9.59528025e-01, 1.11904386e-02],\n",
       "       [4.81990513e-03, 9.86225410e-01, 8.95468439e-03],\n",
       "       [6.70725540e-03, 7.38551351e-01, 2.54741394e-01],\n",
       "       [8.25066411e-03, 9.79395139e-01, 1.23541968e-02],\n",
       "       [1.58765962e-03, 6.30101651e-01, 3.68310689e-01],\n",
       "       [8.30814348e-03, 9.72510584e-01, 1.91812724e-02],\n",
       "       [3.19330686e-03, 4.39998637e-01, 5.56808056e-01],\n",
       "       [8.65557178e-03, 9.79041273e-01, 1.23031556e-02],\n",
       "       [1.10712248e-03, 4.84324289e-01, 5.14568588e-01],\n",
       "       [3.68757294e-03, 9.06114162e-01, 9.01982647e-02],\n",
       "       [4.95773510e-03, 9.82870052e-01, 1.21722132e-02],\n",
       "       [4.39817405e-03, 9.81985545e-01, 1.36162807e-02],\n",
       "       [2.10806529e-03, 9.34856409e-01, 6.30355256e-02],\n",
       "       [1.85544386e-03, 6.96563410e-01, 3.01581146e-01],\n",
       "       [5.11796723e-03, 8.50319113e-01, 1.44562920e-01],\n",
       "       [3.30800724e-02, 9.60580252e-01, 6.33967545e-03],\n",
       "       [8.76972216e-03, 9.70213567e-01, 2.10167108e-02],\n",
       "       [1.30725563e-02, 9.72572458e-01, 1.43549858e-02],\n",
       "       [1.05200431e-02, 9.75348418e-01, 1.41315394e-02],\n",
       "       [5.50957081e-04, 1.80537474e-01, 8.18911569e-01],\n",
       "       [6.76933811e-03, 6.38670081e-01, 3.54560581e-01],\n",
       "       [9.37568093e-03, 9.19005134e-01, 7.16191852e-02],\n",
       "       [3.48832032e-03, 9.55253213e-01, 4.12584663e-02],\n",
       "       [2.46059440e-03, 9.06272977e-01, 9.12664287e-02],\n",
       "       [1.22702256e-02, 9.57183342e-01, 3.05464324e-02],\n",
       "       [7.82966307e-03, 9.13308377e-01, 7.88619595e-02],\n",
       "       [5.54393726e-03, 8.22780094e-01, 1.71675969e-01],\n",
       "       [5.06256636e-03, 9.04614244e-01, 9.03231895e-02],\n",
       "       [7.20503269e-03, 9.72102812e-01, 2.06921557e-02],\n",
       "       [2.98928033e-02, 9.53412397e-01, 1.66947997e-02],\n",
       "       [7.51859952e-03, 9.09311256e-01, 8.31701446e-02],\n",
       "       [1.06453394e-02, 9.65411854e-01, 2.39428062e-02],\n",
       "       [8.91387260e-03, 9.49116568e-01, 4.19695590e-02],\n",
       "       [5.73336768e-03, 9.74851600e-01, 1.94150327e-02],\n",
       "       [8.21415088e-02, 9.09172758e-01, 8.68573355e-03],\n",
       "       [8.78478270e-03, 9.54794491e-01, 3.64207268e-02],\n",
       "       [2.64798581e-06, 1.06948145e-03, 9.98927871e-01],\n",
       "       [1.31147595e-04, 3.50881654e-02, 9.64780687e-01],\n",
       "       [2.90366249e-05, 2.90637531e-02, 9.70907210e-01],\n",
       "       [8.33883990e-05, 4.09199449e-02, 9.58996667e-01],\n",
       "       [1.18819035e-05, 6.78394000e-03, 9.93204178e-01],\n",
       "       [2.40528034e-06, 6.27939827e-03, 9.93718196e-01],\n",
       "       [6.03505941e-04, 6.24990974e-02, 9.36897397e-01],\n",
       "       [1.52658172e-05, 2.87514630e-02, 9.71233271e-01],\n",
       "       [1.57141725e-05, 1.86254200e-02, 9.81358866e-01],\n",
       "       [1.95844419e-05, 1.26573423e-02, 9.87323073e-01],\n",
       "       [1.08933459e-03, 2.95298683e-01, 7.03611982e-01],\n",
       "       [1.29214881e-04, 6.47221679e-02, 9.35148617e-01],\n",
       "       [1.14851089e-04, 6.57408005e-02, 9.34144348e-01],\n",
       "       [5.97016356e-05, 1.64518947e-02, 9.83488404e-01],\n",
       "       [2.27387557e-05, 5.06784269e-03, 9.94909419e-01],\n",
       "       [1.36859532e-04, 3.85391807e-02, 9.61323960e-01],\n",
       "       [2.41095526e-04, 1.14861522e-01, 8.84897383e-01],\n",
       "       [1.80711797e-05, 2.40060817e-02, 9.75975847e-01],\n",
       "       [1.65079868e-07, 6.37536312e-04, 9.99362299e-01],\n",
       "       [2.54173655e-04, 1.30088104e-01, 8.69657722e-01],\n",
       "       [4.64796552e-05, 2.67116983e-02, 9.73241822e-01],\n",
       "       [2.25742409e-04, 3.90360083e-02, 9.60738249e-01],\n",
       "       [1.56217252e-06, 5.57203440e-03, 9.94426403e-01],\n",
       "       [8.51442775e-04, 2.89252449e-01, 7.09896108e-01],\n",
       "       [1.09057259e-04, 5.00625075e-02, 9.49828435e-01],\n",
       "       [1.35600310e-04, 1.36081597e-01, 8.63782803e-01],\n",
       "       [1.44190263e-03, 3.77185764e-01, 6.21372333e-01],\n",
       "       [1.60908503e-03, 3.44573797e-01, 6.53817118e-01],\n",
       "       [1.98916075e-05, 1.11312995e-02, 9.88848809e-01],\n",
       "       [3.13691010e-04, 3.38730888e-01, 6.60955421e-01],\n",
       "       [2.11180686e-05, 3.97721856e-02, 9.60206696e-01],\n",
       "       [1.89231924e-04, 2.37061807e-01, 7.62748961e-01],\n",
       "       [1.30441852e-05, 7.18309622e-03, 9.92803860e-01],\n",
       "       [1.24406861e-03, 4.71515313e-01, 5.27240619e-01],\n",
       "       [1.13917104e-04, 6.94305685e-02, 9.30455514e-01],\n",
       "       [1.26703383e-05, 2.29917192e-02, 9.76995610e-01],\n",
       "       [3.78321203e-05, 1.01068187e-02, 9.89855349e-01],\n",
       "       [2.90157941e-04, 1.15512642e-01, 8.84197200e-01],\n",
       "       [2.08024061e-03, 3.79972544e-01, 6.17947215e-01],\n",
       "       [2.80602471e-04, 1.44342635e-01, 8.55376762e-01],\n",
       "       [2.53642639e-05, 1.25458436e-02, 9.87428792e-01],\n",
       "       [4.72866341e-04, 1.84307680e-01, 8.15219454e-01],\n",
       "       [1.31147595e-04, 3.50881654e-02, 9.64780687e-01],\n",
       "       [1.56064776e-05, 9.77719961e-03, 9.90207194e-01],\n",
       "       [2.07093034e-05, 8.91453057e-03, 9.91064760e-01],\n",
       "       [1.72285096e-04, 6.91666482e-02, 9.30661067e-01],\n",
       "       [2.19853888e-04, 9.78726984e-02, 9.01907448e-01],\n",
       "       [4.10218013e-04, 1.48081919e-01, 8.51507863e-01],\n",
       "       [1.25123940e-04, 2.66992456e-02, 9.73175630e-01],\n",
       "       [6.01332345e-04, 1.29937965e-01, 8.69460703e-01]])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_accuracy = accuracy_score(Y, clf.predict(X))\n",
    "sk_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare accuracy of custom and sklearn algorithm. \n",
      "\n",
      "   accuracy_custom  accuracy_sk  difference\n",
      "0         0.986667         0.98    0.006667\n"
     ]
    }
   ],
   "source": [
    "print(\"Compare accuracy of custom and sklearn algorithm. \\n\")\n",
    "res_compare_test = pd.DataFrame({'accuracy_custom' : [custom_accuracy], 'accuracy_sk' : [sk_accuracy], 'difference' : [abs(custom_accuracy - sk_accuracy)]})\n",
    "print(res_compare_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "834"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
